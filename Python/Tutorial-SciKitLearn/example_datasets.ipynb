{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "from sklearn import datasets, metrics, svm, tree, neighbors\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important types of datasets to have experience with\n",
    "* Task\n",
    "    * Classification (binary and multiclass)\n",
    "    * Regression\n",
    "    * Clustering\n",
    "    * Dimensionality reduction\n",
    "* Input formats:\n",
    "    * Numbers\n",
    "    * Images\n",
    "    * Time series\n",
    "    * Text\n",
    "    * Natural language\n",
    "* Data properties\n",
    "    * Sparse data\n",
    "    * Big data\n",
    "    * Corrupted data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"table_of_contents\"></a>\n",
    "## Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Digits](#sec_digits)\n",
    "* [Iris](#sec_iris)\n",
    "* [Bottom](#sec_bottom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec_digits\"></a>\n",
    "# Digits dataset\n",
    "[top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, summarize the dataset's structure so we know how to explore it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset structure\")\n",
    "print('\\tObject type = ', type(digits))\n",
    "for k, v in digits.items():\n",
    "    t = type(v)\n",
    "    s = v.shape if t == np.ndarray else len(v) if t == list else \"\"\n",
    "    print(f\"\\tKey = {k:15} : {str(t):25} {s}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In agreement with what is described on the datasets [documentation page](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits), there are 1797  images of handwritten digits (i.e. 0-9), consisting of 64 pixels arranged in an 8x8 grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Target names =\", digits.target_names)\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "df = pd.DataFrame(data= np.c_[X, y], columns= digits['feature_names'] + ['target'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits.images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running this takes 10s of seconds\n",
    "n_examples = 3\n",
    "#fig, axs = plt.subplots(10, n_examples)\n",
    "#fig.set_figheight(30)\n",
    "#fig.set_figwidth(3*n_examples)\n",
    "\n",
    "def plot_digit(image, axes=None):\n",
    "    sns.heatmap(image, \n",
    "                annot=False, cbar=False, square=True, \n",
    "                cmap='binary',\n",
    "                ax=axes\n",
    "                ) \n",
    "\n",
    "for num in digits.target_names:\n",
    "    print(f\"{num}...\", end=\"\")\n",
    "    for ii in range(n_examples):\n",
    "        entry = df[df['target']==num].index[ii]\n",
    "        image = digits.images[entry]\n",
    "        #plot_digit(image, axs[num,ii])\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting and predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cv = 5\n",
    "clf = svm.SVC(gamma=0.001, C=100.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = cross_validate(clf, X, y, cv=n_cv)\n",
    "print(result['test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=0, test_size=1/n_cv)\n",
    "p_train = len(X_train) / len(X) * 100\n",
    "p_test = len(X_test) / len(X) * 100\n",
    "print(f'{p_train:.0f}% ({len(X_train)}) training + {p_test:.0f}% ({len(X_test)}) testing : {len(X)} total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "p_right = metrics.accuracy_score(pred, y_test) * 100\n",
    "n_right = metrics.accuracy_score(pred, y_test, normalize=False)\n",
    "n_wrong = len(X_test) - n_right\n",
    "print(f\"{n_wrong}/{len(X_test)} misclassifications ({p_right:.2f}% accurate)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_filt = pred != y_test\n",
    "image_wrong  = X_test[wrong_filt].reshape((-1,8,8))\n",
    "pred_wrong   = pred[wrong_filt]\n",
    "target_wrong = y_test[wrong_filt]\n",
    "wrong_pred_str = [f'{t} => {p}' for p,t in zip(pred_wrong, target_wrong)]\n",
    "sns.countplot(wrong_pred_str)\n",
    "plt.title(\"Tally of mistakes\")\n",
    "plt.xlabel(\"Truth => Prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ii = 2\n",
    "sns.heatmap(image_wrong[ii], annot=False, cbar=False, square=True, cmap='binary')\n",
    "plt.title(f'{target_wrong[ii]} mistaken for {pred_wrong[ii]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<a id=\"sec_iris\"></a>\n",
    "# Iris Dataset\n",
    "[top](#table_of_contents)\n",
    "<img src=\"../Tutorial-DataScience/iris.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SciKit Learn's summary of the data set can be found \n",
    "[here](https://scikit-learn.org/stable/datasets/toy_dataset.html#iris-plants-dataset) \n",
    "and documentation on the `load_iris` function \n",
    "[here](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html). The main task with this dataset is classification of each plant into one of the 3 possible Iris species based on the dimensions of the plant's sepal and petal. We get a hint from the documentation that \"one class is linearly separable from the other 2; the latter are NOT linearly separable from each other\". Therefore this demonstration will have the following goals:\n",
    "\n",
    "* Optimize a classification model with hard-coded linear selections\n",
    "* Improve performance using more advanced machine learning models\n",
    "* Compare to classifications acheived by others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, confirm the `Bunch` class (i.e. extended dictionary type) loaded in by `load_iris` stores the data as specified in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in iris.items():\n",
    "    t = type(v)\n",
    "    s = v.shape if t == np.ndarray else len(v) if t == list else \"\"\n",
    "    print(f\"Key = {k:15} : {str(t):25} {s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, extract needed info from the dataset into convenient formats for further study (e.g. DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data\n",
    "y = iris.target\n",
    "target_id = {n : d for d, n in enumerate(iris.target_names)}\n",
    "df = pd.DataFrame(\n",
    "    data    = np.column_stack((X, y)), \n",
    "    columns = iris.feature_names + ['target_id']\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`target_id` is set to float by default but this is only an integer so change that. Also, add a column for the actual iris type name corresponding to the ID to improve readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.astype({'target_id': 'int64'})\n",
    "df['target_name'] = df.apply(lambda row : iris.target_names[int(row['target_id'])], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Withold part of data for final evaluation to avoid biasing model development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 123\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=random_seed, test_size=0.2)\n",
    "print(f'{len(X_test)}/{len(X)} entries witheld for final model evaluation')\n",
    "\n",
    "def build_dataframe(X, y):\n",
    "    df = pd.DataFrame(\n",
    "        data    = np.column_stack((X, y)), \n",
    "        columns = iris.feature_names + ['target_id']\n",
    "    )  \n",
    "    df = df.astype({'target_id': 'int64'})\n",
    "    df['target_name'] = df.apply(lambda row : iris.target_names[int(row['target_id'])], axis=1)\n",
    "    return df\n",
    "\n",
    "df_train = build_dataframe(X_train, y_train)\n",
    "df_test  = build_dataframe(X_test, y_test)\n",
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With only 4 features, a pairplot is a good way to get an overview for the feature distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_train, vars=iris.feature_names, hue='target_name', corner=True, diag_kind='hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, iris-setosa can be distinguished from the other iris types by petal length alone while the others will benefit from more complex considerations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_by_type = df_train.groupby(['target_name']).min()\n",
    "max_by_type = df_train.groupby(['target_name']).max()\n",
    "\n",
    "min_length = min_by_type.loc['versicolor']['petal length (cm)']\n",
    "max_length = max_by_type.loc['setosa']['petal length (cm)']\n",
    "setosa_petal_length_cutoff = (min_length + max_length) / 2\n",
    "\n",
    "sns.displot(data=df_train, x='petal length (cm)', hue='target_name')\n",
    "plt.title(f\"Setosa Cutoff : petal length < {setosa_petal_length_cutoff:.2f}cm\")\n",
    "plt.axvline(setosa_petal_length_cutoff, color='r', linestyle=':')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets first see how accurate a classifier can be achieved with fixed selections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def optimize_cutoff(cut_feature, iris1, iris2):\n",
    "    min1 = min_by_type.loc[iris1][cut_feature]\n",
    "    min2 = min_by_type.loc[iris2][cut_feature]\n",
    "    max1 = max_by_type.loc[iris1][cut_feature]\n",
    "    max2 = max_by_type.loc[iris2][cut_feature]\n",
    "\n",
    "    # Force iris1 to reference type with  minimum\n",
    "    if min1 > min2:\n",
    "        iris_up, iris_dn = iris1, iris2\n",
    "        min_cutoff, max_cutoff = min1, max2\n",
    "    else:\n",
    "        iris_up, iris_dn = iris2, iris1\n",
    "        min_cutoff, max_cutoff = min2, max1\n",
    "\n",
    "    is_iris_up = df_train['target_name'] == iris_up\n",
    "    is_iris_dn = df_train['target_name'] == iris_dn\n",
    "    total = is_iris_up.sum() + is_iris_dn.sum()\n",
    "\n",
    "    cutoffs = np.linspace(min_cutoff, max_cutoff, 20)\n",
    "    acc = np.zeros(len(cutoffs))\n",
    "\n",
    "    for i, c in enumerate(cutoffs):\n",
    "        true_pos = (is_iris_up & (df_train[cut_feature] >= c)).sum()\n",
    "        true_neg = (is_iris_dn & (df_train[cut_feature] < c)).sum()\n",
    "        acc[i] = (true_pos + true_neg) / total\n",
    "    \n",
    "    optimal_cutoffs = cutoffs[acc == np.max(acc)]\n",
    "    optimal_cutoff = (min(optimal_cutoffs) + max(optimal_cutoffs)) / 2\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(cutoffs, acc, color='k', label='Cutoff accuracy')\n",
    "    ax.set_ylabel('Cutoff accuracy')\n",
    "    ax.set_xlabel(cut_feature)\n",
    "    diff = max_cutoff-min_cutoff\n",
    "    ax.set_xlim(min_cutoff-diff, max_cutoff+diff)\n",
    "    sns.rugplot(data=df_train[is_iris_up | is_iris_dn], \n",
    "                x=cut_feature, \n",
    "                hue='target_name',\n",
    "                linewidth=5,\n",
    "                ax=ax)\n",
    "    ax.axvline(optimal_cutoff, color='r')\n",
    "    ax.set_title(f\"Optimal cutoff for {cut_feature} = {optimal_cutoff:.3f} ({np.max(acc):.1%} acc)\")\n",
    "    plt.show()\n",
    "    \n",
    "    return optimal_cutoff\n",
    "\n",
    "virginica_petal_length_cutoff = optimize_cutoff('petal length (cm)', 'virginica', 'versicolor')\n",
    "virginica_petal_width_cutoff = optimize_cutoff('petal width (cm)', 'virginica', 'versicolor')\n",
    "#optimize_cutoff('petal length (cm)', 'setosa', 'versicolor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both petal width and length provide equal classification power so sticking with petal length simplifies the model. Putting both selections together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_predict(df_X):\n",
    "    y_pred = np.zeros(len(df_X))\n",
    "    \n",
    "    is_setosa = df_X['petal length (cm)'] <= setosa_petal_length_cutoff\n",
    "    is_virginica = df_X['petal length (cm)'] >= virginica_petal_length_cutoff\n",
    "    is_versicolor = ~is_setosa & ~is_virginica\n",
    "    \n",
    "    y_pred[is_setosa]     = target_id['setosa']\n",
    "    y_pred[is_versicolor] = target_id['versicolor']\n",
    "    y_pred[is_virginica]  = target_id['virginica']\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_performance(y, y_pred):\n",
    "    conf_matrix = metrics.confusion_matrix(y, y_pred)\n",
    "    sns.heatmap(data=conf_matrix, \n",
    "                annot=True, \n",
    "                xticklabels=iris.target_names, \n",
    "                yticklabels=iris.target_names)\n",
    "    plt.xlabel('Prediction')\n",
    "    plt.ylabel('Truth')\n",
    "    plt.show()\n",
    "    clf_report = metrics.classification_report(y, y_pred, target_names=iris.target_names)\n",
    "    print(clf_report)\n",
    "\n",
    "print('Performance on training set')\n",
    "print('-'*40)\n",
    "measure_performance(y_train, manual_predict(df_train))\n",
    "\n",
    "print('Performance on test set')\n",
    "print('-'*40)\n",
    "measure_performance(y_test, manual_predict(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An accuracy of 93% is obtained on the test set. Now lets try to do better using more sophisticated techniques. The most obvious ML algorithms to follow up with are support vector machines (SVM), specifically a linear SVM, and decision trees. This is because these algorithm essentially try to do what was done above, namely find simple decision rules that best separate different data classes. \n",
    "\n",
    "The improvement with decision trees is that the algorithms builds combinations of simple rules (i.e. if x > 1 and y < 3 or x > 1 and z == 4, then apply label A) instead of a single rule for each class. Visually, this looks like carving up the phase space into various (potentially disconnected) boxes.\n",
    "\n",
    "The improvement with linear SVM is twofold. First, it can apply cuts to linear combinations of input features, which visually looks like angled boundaries in the phase space instead of vertical or horizontal ones. Second, it combines rules in a more nuanced way than simply giving priority to one rule. Visually, this looks like muultiple bounded reagions with 2 edges as opposed to a single infinitely extending one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.LinearSVC(\n",
    "    dual=False,  \n",
    "    C=1.0, \n",
    "    random_state=random_seed,\n",
    ")\n",
    "clf = make_pipeline(StandardScaler(), clf)\n",
    "#clf = make_pipeline(MinMaxScaler(), clf)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print('Performance on training set')\n",
    "print('-'*40)\n",
    "measure_performance(y_train, clf.predict(X_train))\n",
    "\n",
    "print('Performance on test set')\n",
    "print('-'*40)\n",
    "measure_performance(y_test, clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(\n",
    "    kernel='linear',\n",
    "    C=1.0, \n",
    "    random_state=random_seed,\n",
    ")\n",
    "#clf = make_pipeline(StandardScaler(), clf)\n",
    "#clf = make_pipeline(MinMaxScaler(), clf)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print('Performance on training set')\n",
    "print('-'*40)\n",
    "measure_performance(y_train, clf.predict(X_train))\n",
    "\n",
    "print('Performance on test set')\n",
    "print('-'*40)\n",
    "measure_performance(y_test, clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(\n",
    "    random_state=random_seed,\n",
    ")\n",
    "#clf = make_pipeline(StandardScaler(), clf)\n",
    "#clf = make_pipeline(MinMaxScaler(), clf)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print('Performance on training set')\n",
    "print('-'*40)\n",
    "measure_performance(y_train, clf.predict(X_train))\n",
    "\n",
    "print('Performance on test set')\n",
    "print('-'*40)\n",
    "measure_performance(y_test, clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "tree.plot_tree(clf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = neighbors.KNeighborsClassifier(\n",
    "    n_neighbors=len(iris.target_names)\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print('Performance on training set')\n",
    "print('-'*40)\n",
    "measure_performance(y_train, clf.predict(X_train))\n",
    "\n",
    "print('Performance on test set')\n",
    "print('-'*40)\n",
    "measure_performance(y_test, clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec_bottom\"></a>\n",
    "# Bottom\n",
    "[top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
