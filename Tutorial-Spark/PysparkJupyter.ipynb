{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dd9980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6aaf397",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd93642",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "                    .appName(\"MyApp\") \\\n",
    "                    .master('local[*]') \\\n",
    "                    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f13799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Jupyter\n",
    "%pylab inline\n",
    "!export PYSPARK_DRIVER_PYTHON='jupyter'\n",
    "!export PYSPARK_DRIVER_PYTHON_OPTS='notebook'\n",
    "\n",
    "# If you want to time queries with %%time\n",
    "#spark.conf.set('spark.sql.repl.eagerEval.enabled', True) \n",
    "#spark.conf.set('spark.sql.repl.eagerEval.maxNumRows', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914f017d",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92688b62",
   "metadata": {},
   "source": [
    "The fundamental data structure for PySpark is the **RDD** whose API tries to be as similar to a list as possible.\n",
    "\n",
    "When the entries in your RDD are sufficiently structured (e.g. tuples of the same length and sequence of data types), you can convert it into a **DataFrame** whose API tries to be as similar to a pandas DataFrame as possible.\n",
    "\n",
    "While you can do quite a bit with DataFrame methods that only require passing column names, many processes require providing the DataFrame with a **Column** expression. Column expressions are objects that can reference a column as it is or, for example, a column sorted and cast to a different data type. These expressions get passed to DataFrame methods as if they represented a column that already existed in the DataFrame, though obviously the DataFrame will first have to build the column defined by the column expression before doing anything with it. Column expressions can define a column that is a transformation of each row, a shuffling of all rows, an reduction of rows, a reduction of columns, etc. Column expressions can be built with DataFrame method but are mainly built from method in the `sql.functions` module.\n",
    "\n",
    "Column expressions can be passed not only to DataFrames but also to **GroupedData** objects that result from applying group-by operations to a dataframe. Their behavior is essentially the same.\n",
    "\n",
    "Finally, some Column expressions are built using **WindowSpec** objects thereby leading to a window expression "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d228f39",
   "metadata": {},
   "source": [
    "## RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce75a359",
   "metadata": {},
   "source": [
    "## DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dd2547",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e057dd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d286db5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\n",
    "    'col1': [1, 2, 3],\n",
    "    'col2': [2., 3., 4.],\n",
    "    'col3': ['string1', 'string2', 'string3'],\n",
    "    'col4': [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)],\n",
    "    'col5': [datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)],\n",
    "    'col6': ['2010-07-19T19:12:12.510', '2010-07-19T19:12:12.510', '2010-07-19T19:12:12.510'],\n",
    "})\n",
    "df = spark.createDataFrame(data)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25231ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn('col6', F.to_timestamp('col6')).select('col6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bf2f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# Attributes\n",
    "assert(type(df.schema) == pyspark.sql.types.StructType)\n",
    "\n",
    "assert(df.columns == ['col1', 'col2', 'col3', 'col4', 'col5'])\n",
    "assert(df.dtypes == [\n",
    "    ('col1', 'bigint'),\n",
    "    ('col2', 'double'),\n",
    "    ('col3', 'string'),\n",
    "    ('col4', 'date'),\n",
    "    ('col5', 'timestamp')\n",
    "])\n",
    "print('isStreaming  =',df.isStreaming)\n",
    "print('sql_ctx      =',df.sql_ctx)\n",
    "print('storageLevel =',df.storageLevel)\n",
    "print('isLocal()    =',df.isLocal())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fe9ad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60abb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# Viewing\n",
    "########################################\n",
    "# Print Summaries\n",
    "# df.printSchema()\n",
    "# df.show(first_n_rows, vertical=False)\n",
    "# df.explain()\n",
    "\n",
    "# EDA\n",
    "# df.count()\n",
    "# df.describe()\n",
    "# df.summary()\n",
    "# df.first()\n",
    "# df.head()\n",
    "# df.take()\n",
    "# df.tail()\n",
    "\n",
    "# Selecting Rows\n",
    "# df.filter\n",
    "# df.limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c204cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "F.column(F.col('my_col'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb44b603",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = F.col('col1') ** 2\n",
    "df.select(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f0ea22",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = F.expr('POWER(col1,2)')\n",
    "df.select(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4225a1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = F.col('POWER(col1,2)')\n",
    "df.select(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be5cdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# Column objects and column selection\n",
    "########################################\n",
    "# Creating a column object for selecting\n",
    "assert(type(df['col1']) \n",
    "    == type(df.col1) \n",
    "    == type(F.col('col1')) \n",
    "    == pyspark.sql.Column)\n",
    "assert(df['col1'] is not df.col1) # Column object created on call\n",
    "\n",
    "#########\n",
    "# Creating transformer column objects\n",
    "assert repr(F.col('my_col')) == \"Column<'my_col'>\"\n",
    "\n",
    "# Math\n",
    "assert str(F.col( 'my_col') + 1    ) == str(F.col('(my_col + 1)'))\n",
    "assert str(F.col( 'my_col') * 2 - 1) == str(F.col('((my_col * 2) - 1)'))\n",
    "assert str(F.col( 'my_col') ** 2   ) == str(F.col('POWER(my_col,2)'))\n",
    "#  # cube root\n",
    "\n",
    "# F.abs()\n",
    "# F.round(), F.ceil(), F.floor()\n",
    "# F.bround() # Half even rounding instead of half up\n",
    "# F.exp(), # F.expm1(), F.log(), F.log10(), F.log1p(), F.log2()\n",
    "# F.factorial()\n",
    "# F.pow(), F.sqrt(), F.cbrt()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Trig\n",
    "# F.sin(),   F.cos(),   F.tan()\n",
    "# F.asin(),  F.acos(),  F.atan(), F.atan2()\n",
    "# F.sinh(),  F.cosh(),  F.tanh()\n",
    "# F.asinh(), F.acosh(), F.atanh()\n",
    "# F.degrees(), F.radians()\n",
    "# F.toDegrees(), F.toRadians()\n",
    "# F.hypot()\n",
    "\n",
    "# Stats\n",
    "# F.sum()\n",
    "# F.avg(), F.mean()\n",
    "# F.stddev()\n",
    "# F.stddev_pop()\n",
    "# F.stddev_samp()\n",
    "# F.skewness()\n",
    "# F.corr()\n",
    "# F.covar_pop()\n",
    "# F.covar_samp()\n",
    "\n",
    "# Categorical\n",
    "\n",
    "# Text\n",
    "# F.format_number()\n",
    "# F.format_string()\n",
    "# F.from_csv()\n",
    "# F.from_json()\n",
    "# F.get_json_object()\n",
    "\n",
    "# Sorting\n",
    "# F.asc(),  F.asc_nulls_first(),  F.asc_nulls_last()\n",
    "# F.desc(), F.desc_nulls_first(), F.desc_nulls_last()\n",
    "\n",
    "# Time\n",
    "# F.add_months()\n",
    "# F.current_date()\n",
    "# F.current_timestamp()\n",
    "# F.date_add()\n",
    "# F.date_format()\n",
    "# F.date_sub()\n",
    "# F.date_trunc()\n",
    "# F.datediff()\n",
    "# F.dayofmonth()\n",
    "# F.dayofweek()\n",
    "# F.dayofyear()\n",
    "# F.days()\n",
    "# F.from_unixtime()\n",
    "# F.from_utc_timestamp()\n",
    "# F.minute()\n",
    "# F.month()\n",
    "# F.months()\n",
    "# F.months_between()\n",
    "# F.next_day()\n",
    "# F.second()\n",
    "# F.timestamp_seconds()\n",
    "# F.weekofyear()\n",
    "# F.year()\n",
    "# F.years()\n",
    "\n",
    "# Array\n",
    "# F.array_contains()\n",
    "# F.array_distinct()\n",
    "# F.array_except()\n",
    "# F.array_intersect()\n",
    "# F.array_join()\n",
    "# F.array_max()\n",
    "# F.array_min()\n",
    "# F.array_position()\n",
    "# F.array_remove()\n",
    "# F.array_repeat()\n",
    "# F.array_sort()\n",
    "# F.array_union()\n",
    "# F.arrays_overlap()\n",
    "# F.arrays_zip()\n",
    "# F.element_at()\n",
    "# F.exists()\n",
    "# F.explode()\n",
    "# F.explode_outer()\n",
    "# F.flatten()\n",
    "\n",
    "# Aggregation\n",
    "# F.aggregate()\n",
    "# F.collect_list()\n",
    "# F.collect_set()\n",
    "# F.count()\n",
    "# F.countDistinct()\n",
    "# F.approx_count_distinct() # F.approxCountDistinct() is deprecated\n",
    "# F.first()\n",
    "# F.grouping()\n",
    "# F.grouping_id()\n",
    "\n",
    "# Type conversion\n",
    "# F.ascii()\n",
    "# F.base64()\n",
    "# F.bin()\n",
    "# F.conv()\n",
    "# F.decode()\n",
    "# F.encode()\n",
    "# F.hash()\n",
    "# F.hex()\n",
    "\n",
    "# Multi-column\n",
    "# F.array()\n",
    "# F.coalesce()\n",
    "# F.concat()\n",
    "# F.concat_ws()\n",
    "# F.greatest()\n",
    "\n",
    "# Window\n",
    "# F.cume_dist()\n",
    "# F.dense_rank()\n",
    "\n",
    "# Other\n",
    "# F.assert_true()\n",
    "# F.bitwiseNOT()\n",
    "# F.broadcast()\n",
    "# F.expr()\n",
    "# F.filter()\n",
    "# F.forall()\n",
    "\n",
    "# Unsure\n",
    "# F.bucket()\n",
    "# F.crc32()\n",
    "# F.create_map()\n",
    "# F.functools()\n",
    "\n",
    "# Unsorted\n",
    "# F.hour()\n",
    "# F.hours()\n",
    "# F.last_day()\n",
    "\n",
    "# F.initcap()\n",
    "# F.input_file_name()\n",
    "# F.instr()\n",
    "# F.isnan()\n",
    "# F.isnull()\n",
    "# F.json_tuple()\n",
    "# F.kurtosis()\n",
    "# F.lag()\n",
    "# F.last()\n",
    "# F.lead()\n",
    "# F.least()\n",
    "# F.length()\n",
    "# F.levenshtein()\n",
    "# F.lit()\n",
    "# F.locate()\n",
    "# F.lower()\n",
    "# F.lpad()\n",
    "# F.ltrim()\n",
    "# F.map_concat()\n",
    "# F.map_entries()\n",
    "# F.map_filter()\n",
    "# F.map_from_arrays()\n",
    "# F.map_from_entries()\n",
    "# F.map_keys()\n",
    "# F.map_values()\n",
    "# F.map_zip_with()\n",
    "# F.max()\n",
    "# F.md5()\n",
    "# F.min()\n",
    "# F.monotonically_increasing_id()\n",
    "# F.nanvl()\n",
    "# F.nth_value()\n",
    "# F.ntile()\n",
    "# F.overlay()\n",
    "# F.pandas_udf()\n",
    "# F.percent_rank()\n",
    "# F.percentile_approx()\n",
    "# F.posexplode()\n",
    "# F.posexplode_outer()\n",
    "# F.quarter()\n",
    "# F.raise_error()\n",
    "# F.rand()\n",
    "# F.randn()\n",
    "# F.rank()\n",
    "# F.regexp_extract()\n",
    "# F.regexp_replace()\n",
    "# F.repeat()\n",
    "# F.reverse()\n",
    "# F.rint()\n",
    "# F.row_number()\n",
    "# F.rpad()\n",
    "# F.rtrim()\n",
    "# F.schema_of_csv()\n",
    "# F.schema_of_json()\n",
    "# F.sequence()\n",
    "# F.sha1()\n",
    "# F.sha2()\n",
    "# F.shiftLeft()\n",
    "# F.shiftRight()\n",
    "# F.shiftRightUnsigned()\n",
    "# F.shuffle()\n",
    "# F.signum()\n",
    "# F.since()\n",
    "# F.size()\n",
    "# F.slice()\n",
    "# F.sort_array()\n",
    "# F.soundex()\n",
    "# F.spark_partition_id()\n",
    "# F.split()\n",
    "# F.struct()\n",
    "# F.substring()\n",
    "# F.substring_index()\n",
    "# F.sumDistinct()\n",
    "# F.sys()\n",
    "# F.to_csv()\n",
    "# F.to_date()\n",
    "# F.to_json()\n",
    "# F.to_str()\n",
    "# F.to_timestamp()\n",
    "# F.to_utc_timestamp()\n",
    "# F.transform()\n",
    "# F.transform_keys()\n",
    "# F.transform_values()\n",
    "# F.translate()\n",
    "# F.trim()\n",
    "# F.trunc()\n",
    "# F.udf()\n",
    "# F.unbase64()\n",
    "# F.unhex()\n",
    "# F.unix_timestamp()\n",
    "# F.upper()\n",
    "# F.var_pop()\n",
    "# F.var_samp()\n",
    "# F.variance()\n",
    "# F.warnings()\n",
    "# F.when()\n",
    "# F.window()\n",
    "# F.xxhash64()\n",
    "# F.zip_with()\n",
    "\n",
    "# Modifying a column object\n",
    "\n",
    "# Selecting a column\n",
    "col1_expr = df['col1']\n",
    "assert(type(df.select('col1'))\n",
    "    == type(df.select(col1_expr))\n",
    "    == type(df[['col1']])\n",
    "    == pyspark.sql.dataframe.DataFrame)\n",
    "\n",
    "col1_df = df.select('col1')\n",
    "col1 = [row['col1'] for row in col1_df.collect()]\n",
    "assert col1 == data['col1'].tolist()\n",
    "assert all(col1_df.toPandas() == data[['col1']])\n",
    "#df.selectExpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92838f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "str(F.col( 'my_col') * 2 - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538efc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# Modifying\n",
    "########################################\n",
    "# Add column\n",
    "# df.withColumn\n",
    "# df.withColumnRenamed\n",
    "\n",
    "# Data Cleaning\n",
    "assert(df.na.df is df)\n",
    "#df.replace # (df.na.replace)\n",
    "#df.fillna # (df.na.fill)\n",
    "#df.drop\n",
    "#df.dropDuplicates # (df.drop_duplicates)\n",
    "#df.dropna # (df.na.drop)\n",
    "\n",
    "# Conversions\n",
    "#df.rdd\n",
    "#df.toJSON()\n",
    "#df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33c6963",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c8533d",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# Calculations\n",
    "########################################\n",
    "# Aggregating\n",
    "#df.agg()\n",
    "print(df.agg())\n",
    "print(F.avg('col1'))\n",
    "\n",
    "# Stats\n",
    "assert(df.stat.df is df)\n",
    "#df.cov (df.stat.cov)\n",
    "#df.approxQuantile (df.stat.approxQuantile)\n",
    "#df.sampleBy (df.stat.sampleBy)\n",
    "#df.corr (df.stat.corr)\n",
    "#df.freqItems (df.stat.freqItems)\n",
    "#df.crosstab (df.stat.crosstab)\n",
    "\n",
    "# Functions\n",
    "from pyspark.sql import functions\n",
    "\n",
    "# Apply user-defined functions (UDF)\n",
    "# df.mapInPandas\n",
    "# @pandas_udf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430df94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# Grouping\n",
    "########################################\n",
    "#df.groupBy # (df.groupby)\n",
    "df_grp = df.groupBy('col1')\n",
    "assert(type(df_grp) == pyspark.sql.group.GroupedData)\n",
    "\n",
    "# Group reduce\n",
    "# df_grp.count()\n",
    "# df_grp.sum()\n",
    "# df_grp.avg() # (df_grp.mean())\n",
    "# df_grp.min()\n",
    "# df_grp.max()\n",
    "# df_grp.agg()\n",
    "# df_grp.apply()\n",
    "# df_grp.applyInPandas()\n",
    "\n",
    "# Regroup\n",
    "# df_grp.pivot()\n",
    "# df_grp.cogroup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55273b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# SQL\n",
    "########################################\n",
    "#df.createGlobalTempView\n",
    "#df.createOrReplaceTempView\n",
    "#df.createTempView\n",
    "#spark.udf.register()\n",
    "#spark.sql()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86a3d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# I/O\n",
    "########################################\n",
    "#df.write\n",
    "#df.writeTo\n",
    "#df.writeStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c86abca",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# Unsorted methods\n",
    "########################################\n",
    "# Same name as RDD\n",
    "# df.sample\n",
    "# df.toDF\n",
    "# df.randomSplit\n",
    "# df.toLocalIterator\n",
    "# df.checkpoint\n",
    "# df.cache\n",
    "# df.unpersist\n",
    "# df.persist\n",
    "# df.join\n",
    "# df.distinct\n",
    "# df.union\n",
    "# df.subtract\n",
    "# df.repartition\n",
    "# df.coalesce\n",
    "# df.foreach\n",
    "# df.foreachPartition\n",
    "\n",
    "# Not in RDD\n",
    "# df.agg\n",
    "# df.alias\n",
    "# df.colRegex\n",
    "# df.crossJoin\n",
    "# df.cube\n",
    "# df.exceptAll\n",
    "# df.hint\n",
    "# df.inputFiles\n",
    "# df.intersect\n",
    "# df.intersectAll\n",
    "# df.orderBy\n",
    "# df.registerTempTable\n",
    "# df.repartitionByRange\n",
    "# df.rollup\n",
    "# df.sameSemantics\n",
    "# df.semanticHash\n",
    "# df.sort\n",
    "# df.sortWithinPartitions\n",
    "# df.transform\n",
    "# df.unionAll\n",
    "# df.unionByName\n",
    "# df.where\n",
    "# df.withWatermark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da25115f",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937276aa",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0759b845",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472dbdbe",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d99843a",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3021ec8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1 (main, Dec 23 2022, 09:28:24) [Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
